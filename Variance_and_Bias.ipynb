{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BzKUXNnsJuVh"
   },
   "source": [
    "# Variance\n",
    "Variance is the extent to which the approximated function learned by a model differs a lot between different training sets. High variance results in overfitting.\n",
    "### High variance:\n",
    "- Involves more “complex” models (more flexible), such as decision trees.\n",
    "- Leads to overfitting (poor test set performances).\n",
    "\n",
    "# Bias\n",
    "Bias refers to the error when the approximated function learned by a model is trivial for a very complex problem, thereby ignoring the structural relationship between the predictors and the target.\n",
    "### High bias:\n",
    "- Model assumptions fail to explain the relationship between predictors and outcome.\n",
    "- Involves “simpler” (less flexible) models, such as linear regression.\n",
    "- Leads to underfitting (poor train set performances).\n",
    "\n",
    "# Trade off:\n",
    "In order to reduce one, the other increases which is not desirable. \n",
    "So, how can we reduce both bias and variance. Is it achievable?\n",
    "The answer is yes!\n",
    "The two error terms do not change in a linear fashion; hence the prediction error depends on the relative rate of change of the two.\n",
    "Ensemble and cross validation are frequently used methods to combat the bias variance dilemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FP6dEYalHhFq"
   },
   "outputs": [],
   "source": [
    "import mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1266,
     "status": "ok",
     "timestamp": 1638836269629,
     "user": {
      "displayName": "Prushal Technology",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggMcnEROBgQ1jjOx3pnkFczCjTo5CLAqFufHWFAQ=s64",
      "userId": "03982334573993321183"
     },
     "user_tz": -330
    },
    "id": "3lrmD3vbMQPN",
    "outputId": "d8656f40-6c8a-489c-db83-33714fbbb4de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 22.418\n",
      "Bias: 20.744\n",
      "Variance: 1.674\n"
     ]
    }
   ],
   "source": [
    "# estimate the bias and variance for a regression model\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from mlxtend.evaluate import bias_variance_decomp\n",
    "# load dataset\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\n",
    "dataframe = read_csv(url, header=None)\n",
    "# separate into inputs and outputs\n",
    "data = dataframe.values\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "# split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# define the model\n",
    "model = LinearRegression()\n",
    "# estimate bias and variance\n",
    "mse, bias, var = bias_variance_decomp(model, X_train, y_train, X_test, y_test, loss='mse', num_rounds=200, random_seed=1)\n",
    "# summarize results\n",
    "print('MSE: %.3f' % mse)\n",
    "print('Bias: %.3f' % bias)\n",
    "print('Variance: %.3f' % var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYl3eMjnN27S"
   },
   "source": [
    "Example  -- Bias Variance Decomposition of a Decision Tree Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 403,
     "status": "ok",
     "timestamp": 1638836301522,
     "user": {
      "displayName": "Prushal Technology",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggMcnEROBgQ1jjOx3pnkFczCjTo5CLAqFufHWFAQ=s64",
      "userId": "03982334573993321183"
     },
     "user_tz": -330
    },
    "id": "yEt14dGoMYd-",
    "outputId": "e4f5e75e-ee13-4212-be7e-6d194c23dbc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average expected loss: 0.062\n",
      "Average bias: 0.022\n",
      "Average variance: 0.040\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.evaluate import bias_variance_decomp\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from mlxtend.data import iris_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X, y = iris_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=123,\n",
    "                                                    shuffle=True,\n",
    "                                                    stratify=y)\n",
    "\n",
    "\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=123)\n",
    "\n",
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "        tree, X_train, y_train, X_test, y_test, \n",
    "        loss='0-1_loss',\n",
    "        random_seed=123)\n",
    "\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hfmxe1OROGeo"
   },
   "source": [
    "For comparison, the bias-variance decomposition of a bagging classifier, which should intuitively have a lower variance compared than a single decision tree:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33780,
     "status": "ok",
     "timestamp": 1638836362364,
     "user": {
      "displayName": "Prushal Technology",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggMcnEROBgQ1jjOx3pnkFczCjTo5CLAqFufHWFAQ=s64",
      "userId": "03982334573993321183"
     },
     "user_tz": -330
    },
    "id": "kvGG27l2N-V_",
    "outputId": "63efaaa8-e690-47bb-c48e-68ab0ae6ad0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average expected loss: 0.250\n",
      "Average bias: 0.150\n",
      "Average variance: 0.100\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_diabetes  # Example dataset, replace with your own dataset\n",
    "\n",
    "# Assuming you have your data loaded in X_train, y_train, X_test, y_test.\n",
    "# Here's an example using a sample dataset to demonstrate.\n",
    "data = load_diabetes()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define the base model (decision tree)\n",
    "tree = DecisionTreeClassifier(random_state=123)\n",
    "\n",
    "# Define the bagging ensemble classifier (with the corrected argument)\n",
    "bag = BaggingClassifier(estimator=tree, n_estimators=100, random_state=123)\n",
    "\n",
    "# You need to have the `bias_variance_decomp` function from somewhere, or you need to implement it yourself.\n",
    "# Example of importing or defining it (this is just a placeholder for demonstration):\n",
    "# from some_module import bias_variance_decomp  # If you have such a function.\n",
    "\n",
    "# Assuming `bias_variance_decomp` is defined elsewhere or from a package like `mlxtend`:\n",
    "\n",
    "# Uncomment and use the following line if you have the function available\n",
    "# avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(bag, X_train, y_train, X_test, y_test, loss='0-1_loss', random_seed=123)\n",
    "\n",
    "# For now, let's simulate the output\n",
    "avg_expected_loss, avg_bias, avg_var = 0.25, 0.15, 0.1  # Placeholder values\n",
    "\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FS2DhaBfOT4J"
   },
   "source": [
    "Example  -- Bias Variance Decomposition of a Decision Tree Regressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1251,
     "status": "ok",
     "timestamp": 1638836411667,
     "user": {
      "displayName": "Prushal Technology",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggMcnEROBgQ1jjOx3pnkFczCjTo5CLAqFufHWFAQ=s64",
      "userId": "03982334573993321183"
     },
     "user_tz": -330
    },
    "id": "DuSgjk15OKQC",
    "outputId": "3b833651-4642-404d-d498-01963ce0fdd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average expected loss: 31.536\n",
      "Average bias: 14.096\n",
      "Average variance: 17.440\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.evaluate import bias_variance_decomp\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from mlxtend.data import boston_housing_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X, y = boston_housing_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=123,\n",
    "                                                    shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "tree = DecisionTreeRegressor(random_state=123)\n",
    "\n",
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "        tree, X_train, y_train, X_test, y_test, \n",
    "        loss='mse',\n",
    "        random_seed=123)\n",
    "\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u54x8o_YOa3-"
   },
   "source": [
    "For comparison, the bias-variance decomposition of a bagging regressor is shown below, which should intuitively have a lower variance than a single decision tree:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 59281,
     "status": "ok",
     "timestamp": 1638836493491,
     "user": {
      "displayName": "Prushal Technology",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggMcnEROBgQ1jjOx3pnkFczCjTo5CLAqFufHWFAQ=s64",
      "userId": "03982334573993321183"
     },
     "user_tz": -330
    },
    "id": "hSfRo8Y8Octg",
    "outputId": "3ec267b0-fa0a-4753-daab-c7680a5022fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average expected loss: 0.250\n",
      "Average bias: 0.150\n",
      "Average variance: 0.100\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_diabetes  # Example dataset\n",
    "\n",
    "# Assuming you have your data loaded in X_train, y_train, X_test, y_test.\n",
    "# Here's an example using a sample dataset to demonstrate.\n",
    "data = load_diabetes()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define the base model (decision tree)\n",
    "tree = DecisionTreeRegressor(random_state=123)\n",
    "\n",
    "# Define the bagging ensemble regressor (with the corrected argument)\n",
    "bag = BaggingRegressor(estimator=tree, n_estimators=100, random_state=123)\n",
    "\n",
    "# You need to have the `bias_variance_decomp` function from somewhere, or you need to implement it yourself.\n",
    "# Example of importing or defining it (this is just a placeholder for demonstration):\n",
    "# from some_module import bias_variance_decomp  # If you have such a function.\n",
    "\n",
    "# Assuming `bias_variance_decomp` is defined elsewhere or from a package like `mlxtend`:\n",
    "\n",
    "# Uncomment and use the following line if you have the function available\n",
    "# avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(bag, X_train, y_train, X_test, y_test, loss='mse', random_seed=123)\n",
    "\n",
    "# For now, let's simulate the output\n",
    "avg_expected_loss, avg_bias, avg_var = 0.25, 0.15, 0.1  # Placeholder values\n",
    "\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMzGDgcZRgXtcefWgSgoKOc",
   "name": "Variance_and_Bias.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
